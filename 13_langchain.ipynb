{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables"
      ],
      "metadata": {},
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import urllib\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from typing import List\n",
        "from operator import itemgetter\n",
        "\n",
        "# LangChain Imports needed\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "\n",
        "# Our own libraries needed\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "from common.utils import get_search_results\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials2.env\")\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1716463039194
        }
      },
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"what is azure ml for\""
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1716463042017
        }
      },
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1716463043132
        }
      },
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A gentle intro to chaining LLMs and prompt engineering"
      ],
      "metadata": {},
      "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.\n",
        "\n",
        "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
        "\n",
        "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
        "\n",
        "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
        "\n",
        "Here’s an example:"
      ],
      "metadata": {},
      "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69"
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETION_TOKENS = 2000\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0.5, \n",
        "                      max_tokens=COMPLETION_TOKENS)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1716463047172
        }
      },
      "id": "13df9247-e784-4e04-9475-55e672efea47"
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}. Give your response in {language}\")\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1716463047469
        }
      },
      "id": "a3b55adb-6f98-4f15-b67a-9fbba5820560"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component."
      ],
      "metadata": {},
      "id": "6417d052-0035-4635-93e8-2bd3ec50d796"
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1716463049156
        }
      },
      "id": "77a37e60-a1ef-4750-a1ec-9e4fe5ba07fa"
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": QUESTION, \"language\": \"Spanish\"})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "'Azure ML, que es una abreviatura de Azure Machine Learning, es una plataforma de servicios en la nube de Microsoft que se utiliza para la creación, implementación y gestión de soluciones de aprendizaje automático. Está diseñado para ayudar a los desarrolladores y científicos de datos a crear, entrenar, probar, implementar y administrar modelos de aprendizaje automático en la nube.\\n\\nAzure ML proporciona un entorno de trabajo que ofrece una amplia gama de herramientas y servicios. Estos permiten a los usuarios y las organizaciones desarrollar modelos de aprendizaje automático en gran escala, lo que facilita la creación de aplicaciones y servicios inteligentes.\\n\\nAdemás, Azure ML también proporciona características que permiten la colaboración y la gestión de flujos de trabajo, lo que facilita la implementación y el mantenimiento de soluciones de aprendizaje automático. En resumen, Azure ML es una plataforma completa para el desarrollo y la implementación de soluciones de aprendizaje automático.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463067206
        }
      },
      "id": "50ea1a39-bea2-4f18-bcc0-bd4eb2e01675"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
        "\n",
        "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
      ],
      "metadata": {},
      "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The second type of Chains are Utility:**\n",
        "\n",
        "* Utility — These are specialized chains, comprised of many building blocks to help solve a specific task. For example, LangChain supports some end-to-end chains (such as `create_retrieval_chain` for QnA Doc retrieval, Summarization, etc).\n",
        "\n",
        "We will build our own specific chain in this workshop for digging deeper and solve our use case of enhancing the results of Azure AI Search."
      ],
      "metadata": {
        "tags": []
      },
      "id": "12c48038-b1af-4228-8ffb-720e554fd3b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So really, our only job now is to make sure that the results from the Azure AI Search queries fit on the LLM context size, and then let it do its magic."
      ],
      "metadata": {},
      "id": "80e79235-3d8b-4713-9336-5004cc4a1556"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a Prompt Template that will ground the response only in the given context."
      ],
      "metadata": {},
      "id": "235d4238-df6e-40eb-ab38-4fe6db614acd"
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question thoroughly, based **ONLY** on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1716463072167
        }
      },
      "id": "f86ed786-aca0-4e25-947b-d9cf3a82665c"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question thoroughly, based **ONLY** on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463073220
        }
      },
      "id": "919edc42-2c25-4407-84d3-e9e86046801e"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# Creation of our custom chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "chain.invoke({\"question\": \"who is Rich Seelinger?\", \"context\": \"Rich Seelinger, CEO and Chief Claims Officer of Enstar US.\"})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 17.8 ms, sys: 0 ns, total: 17.8 ms\nWall time: 1.28 s\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "'Rich Seelinger is the CEO and Chief Claims Officer of Enstar US.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {},
      "id": "25cba3d1-b5ab-4e28-96b3-ef923d99dc9f"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from typing import Dict, List, Optional, Type\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.agents import AgentExecutor, Tool, create_openai_tools_agent\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.utilities import BingSearchAPIWrapper\n",
        "\n",
        "from common.callbacks import StdOutCallbackHandler\n",
        "from common.prompts import BINGSEARCH_PROMPT\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string.replace(\"$\",\"USD \")))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials2.env\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463625105
        }
      },
      "id": "dceab3d9-5a9e-42cb-bc95-68b85c10f1ef"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463625268
        }
      },
      "id": "2f7c618e-5d5d-4c6d-8fcc-9debf84227d8"
    },
    {
      "cell_type": "code",
      "source": [
        "cb_handler = StdOutCallbackHandler()\n",
        "cb_manager = CallbackManager(handlers=[cb_handler])\n",
        "\n",
        "COMPLETION_TOKENS = 2000\n",
        "\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0.5, max_tokens=COMPLETION_TOKENS, \n",
        "                      streaming=True, callback_manager=cb_manager)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463625486
        }
      },
      "id": "f22ed7aa-4e48-4488-b80c-63a35e733b33"
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchInput(BaseModel):\n",
        "    query: str = Field(description=\"should be a search query\")\n",
        "\n",
        "class MyBingSearch(BaseTool):\n",
        "    \"\"\"Tool for a Bing Search Wrapper\"\"\"\n",
        "    \n",
        "    name = \"Searcher\"\n",
        "    description = \"useful to search the internet.\\n\"\n",
        "    args_schema: Type[BaseModel] = SearchInput\n",
        "\n",
        "    k: int = 5\n",
        "    \n",
        "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
        "        bing = BingSearchAPIWrapper(k=self.k)\n",
        "        return bing.results(query,num_results=self.k)\n",
        "            \n",
        "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
        "        bing = BingSearchAPIWrapper(k=self.k)\n",
        "        loop = asyncio.get_event_loop()\n",
        "        results = await loop.run_in_executor(ThreadPoolExecutor(), bing.results, query, self.k)\n",
        "        return results"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463625667
        }
      },
      "id": "97a3eb01-904b-401a-9f7b-f39aa77d311e"
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_html(content) -> str:\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_content_with_links = soup.get_text()\n",
        "    return text_content_with_links\n",
        "\n",
        "def fetch_web_page(url: str) -> str:\n",
        "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0'}\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    return parse_html(response.content)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463625902
        }
      },
      "id": "f3a9dd6f-0762-40a9-b212-a9f7138ab64b"
    },
    {
      "cell_type": "code",
      "source": [
        "web_fetch_tool = Tool.from_function(\n",
        "    func=fetch_web_page,\n",
        "    name=\"WebFetcher\",\n",
        "    description=\"useful to fetch the content of a url\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463627163
        }
      },
      "id": "2d9d6efb-ee00-4271-9282-1a513185dd1e"
    },
    {
      "cell_type": "code",
      "source": [
        "# tools = [MyBingSearch(k=5), web_fetch_tool] # With GPT-4 you can add the web_fetch_tool\n",
        "\n",
        "tools = [MyBingSearch(k=5)] # With GPT-3.5 \n",
        "\n",
        "prompt = BINGSEARCH_PROMPT\n",
        "\n",
        "# Construct the OpenAI Tools agent\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, \n",
        "                               return_intermediate_steps=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463627331
        }
      },
      "id": "23aab0bd-11f2-4318-8e10-33469e37c945"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.tools"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "[MyBingSearch()]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463627550
        }
      },
      "id": "e0bc94fb-85d1-444c-9097-6fce420b4e85"
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION = \"Create a list with the main facts on What is happening with the oil supply in the world right now?\"\n",
        "# QUESTION = \"How much is 50 USD in Euros and is it enough for an average hotel in Madrid?\"\n",
        "# QUESTION = \"My son needs to build a pinewood car for a pinewood derbi, how do I build such a car?\"\n",
        "# QUESTION = \"I'm planning a vacation to Greece, tell me budget for a family of 4, in Summer, for 7 days including travel, lodging and food costs\"\n",
        "# QUESTION = \"Who won the 2023 superbowl and who was the MVP?\"\n",
        "QUESTION = \"\"\"\n",
        "compare the number of job opennings (provide the exact number), the average salary within 15 miles of Dallas, TX, for these ocupations:\n",
        "\n",
        "- ADN Registerd Nurse \n",
        "- Occupational therapist assistant\n",
        "- Dental Hygienist\n",
        "- Graphic Designer\n",
        "\n",
        "\n",
        "# Create a table with your findings. Place the sources on each cell.\n",
        "# \"\"\""
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463628062
        }
      },
      "id": "890b4270-834a-499d-907c-c4396de4bc99"
    },
    {
      "cell_type": "code",
      "source": [
        "async for chunk in agent_executor.astream({\"question\": QUESTION}):\n",
        "    # Agent Action\n",
        "    if \"actions\" in chunk:\n",
        "        for action in chunk[\"actions\"]:\n",
        "            print(f\"Calling Tool: `{action.tool}` with input `{action.tool_input}`\")\n",
        "    # Observation\n",
        "    elif \"steps\" in chunk:\n",
        "        # Uncomment if you need to have the information retrieve from the tool\n",
        "        # for step in chunk[\"steps\"]:\n",
        "        #     print(f\"Tool Result: `{step.observation}`\")\n",
        "        continue\n",
        "    # Final result\n",
        "    elif \"output\" in chunk:\n",
        "        # No need to print the final output again since we would be streaming it as it is produced\n",
        "        # print(f'Final Output: {chunk[\"output\"]}') \n",
        "        continue\n",
        "    else:\n",
        "        raise ValueError()\n",
        "    print(\"---\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1716463659068
        }
      },
      "id": "384faabb-2de3-4283-98f8-e1d828987114"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "##### By using OpenAI, the answers to user questions are way better than taking just the results from Azure AI Search. So the summary is:\n",
        "- Utilizing Azure AI Search, we conduct a multi-index hybrid search that identifies the top chunks of documents from each index.\n",
        "- Subsequently, Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.\n",
        "- Best of two worlds!\n",
        "\n",
        "##### Important observations on this notebook:\n",
        "\n",
        "1) Answers with GPT-3.5 are less quality but fast\n",
        "2) Answers with GPT-3.5 sometimes failed on provinding citations in the right format\n",
        "3) Answers with GPT-4 are great quality but slower\n",
        "4) Answers with GPT-4 always provide good and diverse citations in the right format\n",
        "5) Models like Mistral Large or Cohere Command R+ provide, so far, a similar experience to GPT-4\n",
        "5) Streaming the answers improves the user experience big time!"
      ],
      "metadata": {},
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
      ],
      "metadata": {},
      "id": "fdc6e2fe-1c34-4952-99ad-14940f022379"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}