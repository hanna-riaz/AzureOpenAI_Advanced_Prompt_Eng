{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "import os \n",
        "from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "default_credential=ManagedIdentityCredential(client_id=\"d30cba06-04c1-4065-a91d-8b7ce3b07b78\")\n",
        "token=default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
        "Resource_endpoint=\"https://openaiykus.openai.azure.com/\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint = Resource_endpoint, \n",
        "  api_key=token.token,  \n",
        "  api_version=\"2023-05-15\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700428367245
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Temperature\n",
        "\n",
        "Defaults to 1, Optional\n",
        "\n",
        "What sampling temperature to use, between 0 and 2. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\n",
        "\n",
        "We generally recommend altering this or top_p but not both."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def call_openai(num_times, start_phrase, temperature):\n",
        "    for i in range(num_times):\n",
        "        \n",
        "        deployment_name='gpt-35-turbo-instruct' \n",
        "\n",
        "        # Send a completion call to generate an answer\n",
        "        response = client.completions.create(\n",
        "            model=deployment_name, \n",
        "            prompt=start_phrase, \n",
        "            temperature=temperature)\n",
        "        print(response.choices[0].text)\n",
        "        print(\"*****************************\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1700428849945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call_openai(10, 'Royal London is a ', temperature = 0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700428854877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call_openai(10, 'Royal London is a ', temperature = 2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n*****************************\n1544 departure British criminal\n\nLeader\nby-electionf about getting $312380\n*****************************\n117 lift slender take quite different some optimistic cost assumptions be enchanted etc years an additional\n*****************************\n28-bed acute detox ULOC*\n\nGreater Londra Royal opust got trē\n*****************************\n76 Perkins Gaming aka LDSP East Essex press out this site has a list presented\n*****************************\n157 eyes\n\nhardyc.\n\niv Peiping rode king Cole get priority Piers\n*****************************\n18g thinner-gr handle for pur | Partystore Gymtime BS\",\nThe instruction\n*****************************\n160 wet spring one look UK tea concentrate blended from decige copupulent eight\n*****************************\n06-point circuit horse point Ilona De Bootmaestate.\n\nVejover Ol\n*****************************\ndistinct section uploaded every Wed(URL HIDDEN)Con Suite Royale Simplex M-L\n*****************************\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700428859878
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Top_p\n",
        "\n",
        "Defaults to 1, Optional\n",
        "\n",
        "top_p parameter which stands for “top probability” and an alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. \n",
        "\n",
        "The top_p refers to the probability mass that should be used when considering the next word in the generated text. Essentially it ** sets a threshold for the probability of the next word being chosen and only considers the most likely words that exceed that threshold.**\n",
        "\n",
        "We generally recommend altering this or temperature but not both."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def call_openai(num_times, start_phrase, top_p):\n",
        "    for i in range(num_times):\n",
        "        \n",
        "        deployment_name='gpt-35-turbo-instruct' \n",
        "\n",
        "        # Send a completion call to generate an answer\n",
        "        response = client.completions.create(\n",
        "            model=deployment_name, \n",
        "            prompt=start_phrase, \n",
        "            top_p=top_p)\n",
        "        print(response.choices[0].text)\n",
        "        print(\"*****************************\")"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1700428707267
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call_openai(10, 'Royal London is a ', top_p = 1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "150 year old life insurance and pensions company, based in central London. Over that\n*****************************\n95-seat meeting room\n\nLondon \n*****************************\n100% online platform. A mutual insurance company formed back in 1861 in\n*****************************\n100% mutual organisation, this means that it doesn't have any shareholders that it\n*****************************\n158 year-old mutual which offers pensions, investment and life cover products.\n\nSaturday essay\n*****************************\n55-acre park and 'proper' little village tucked deep in Austin. And,\n*****************************\n1995 album by British singer/songwriter Graham Parker. The album was Parker\n*****************************\n352-bed independent hospital that has been providing high quality private healthcare to the people of\n*****************************\n50 years old company with its roots in UAE, with assets under management of A\n*****************************\n1,000 square-metre bar, restaurant and live music venue that gives you\n*****************************\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1700428713872
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "call_openai(10, 'Royal London is a ', top_p = 0.1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "200-year-old mutual life and pensions company. We are committed to helping our customers\n*****************************\n200-year-old mutual life and pensions company. We are committed to helping our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n200-year-old mutual life and pensions company. We are committed to helping our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n200-year-old mutual life and pensions company. We are committed to helping our customers\n*****************************\n100% mutual life, pensions and investment company. We’re owned by our customers\n*****************************\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1700428726870
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max_Tokens \n",
        "\n",
        "Default value=16, Optional\n",
        "\n",
        "The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens can't exceed the model's context length. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployment_name='gpt-35-turbo-instruct' \n",
        "#This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
        "    \n",
        "# Send a completion call to generate an answer\n",
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    temperature=1,\n",
        "    max_tokens=5)\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\n  \"id\": \"cmpl-8MkQiyo6bMHVvvjO9YwQkNM3NuRdn\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"7-story hotel in a\"\n    }\n  ],\n  \"created\": 1700432384,\n  \"model\": \"gpt-35-turbo-instruct\",\n  \"object\": \"text_completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 5,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 10\n  }\n}\n"
        }
      ],
      "execution_count": 64,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432383815
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send a completion call to generate an answer\n",
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    temperature=1,\n",
        "    max_tokens=1000)\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\n  \"id\": \"cmpl-8MkR6AhbbAuEFeAWOUL02JAsP9R7f\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"5* rated financial services organisation - with key\\n\\nservices offering protection, pension and investment solutions.\\n\\nI am supporting their Motor services team - whose purpose is to support\\n\\nUK customers with their Motor claims/investments.\\n\\nThis team requires an Incident/Customer Services Analyst to join their\\n\\ngrowing function and assist with data handling and analysis - in\\n\\nrelation to accidents/incidents that occur when UK customers are driving\\n\\nlocally or in europe.\\n\\nSpecifically the role will involve handling client data for:\\n\\n- Accidents / incidents involving policy holders across europe\\n\\n- Translations and language analysis\\n\\n- Busines analysis\\n\\nFull training will be provided - so no prior claims (or insurance)\\n\\nexperience is required.\\n\\nThe specific location is Wilmslow - near Manchester.\\n\\nPotential travel / working from home opportunities after a short period\\n\\nof training/onboarding.\\n\\nGreat starting salary on offer (and benefits including 12% bonus), and\\n\\noppen for future career progression within the organisation.\\n\\nCandidates for this role must:\\n\\n- Live locally to Wilmslow / Cheshire: within daily commuting distance\\n\\n- Be comfortable and excel at handling and analysing data\\n\\n- Have typically a background in analysis/data extraction/processing as\\n\\na core competency\\n\\n- Be willing to travel to europe infrequently for a week at a time\\n\\n- Be comfortable interacting with customers if/when required (native\\n\\neuropean language a big bonus: german, french, spanish, dutch\\n\\nespecially)\\n\\n- Be interested in working for a highly rated financial services\\n\\norganisation with a competitive salary and bonus\\n\\nInterested? Please reply with an updated CV for immediate review\\n\\nThe candidate must have FULL VALID WORKING RIGHTS for the U.K. The\\n\\ncompany will NOT SPONSOR WORK PERMITS for applicants wishing to work in\\n\\nthe U.K. Unfortunately, due to the anticipated high level of response it\\n\\nwill not be possible to follow up on all applications. Candidates who\\n\\nhave been referred to us by a recruitment consultancy or a third party\\n\\n(e.g. a colleague or client) may be included in this search, but will\\n\\nonly be contacted for this vacancy.\\n\\nhttp://www.passion4job.com/\\n\\nThe messages are owned by the Sport4relo LLC and directed to its members. It is of no interest to any other parties. If you have any doubts concerning copyrights please contact [email protected]\\n\\nThis service brought to you by Sport4relo LLC\\n\\n925 N. Michigan Avenue, Suite 4600, Chicago, IL\"\n    }\n  ],\n  \"created\": 1700432408,\n  \"model\": \"gpt-35-turbo-instruct\",\n  \"object\": \"text_completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 501,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 506\n  }\n}\n"
        }
      ],
      "execution_count": 65,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432413936
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# n\n",
        "\n",
        "Defaults to 1, optional\n",
        "\n",
        "How many completions to generate for each prompt. To generate multiple completions, we specify the n request parameter, which simply stands for ** “number of completions” **\n",
        "\n",
        "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "deployment_name='gpt-35-turbo-instruct' \n",
        "#This will correspond to the custom name you chose for your deployment when you deployed a model. \n",
        "    \n",
        "# Send a completion call to generate an answer\n",
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    temperature=1,\n",
        "    n=3)\n",
        "\n",
        "for i in response.choices:\n",
        "    print(i.text)\n",
        "    print (\"**************************\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "5 star London's Premier commercial aware moving & storage specialist company.Our service provides the\n**************************\n50 storey mixed use tower in the Isle of Dogs Docklands. It provides\n**************************\n1950 British Comedy Drama film directed by Herbert Wilcox and starring Anna Neagle\n**************************\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700429309879
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# logprobs\n",
        "\n",
        "Defaults to null, optional\n",
        "\n",
        "**Include the log probabilities on the logprobs most likely tokens**, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.\n",
        "\n",
        "** tokens ** — is an array of tokens generated by the language model. Each token is a word or part of a word.\n",
        "\n",
        "** token_logprobs ** — represents an array of log probabilities for each token in the tokens array. Log probability indicates the likelihood of the language model generating that token for the given prompt. The logprob values are negative, where smaller (more negative) numbers indicate a less likely outcome.\n",
        "\n",
        "** top_logprobs ** — represents an array of log probability objects, representing tokens most likely to be used for the completion. For example, if we specify the request parameter top_p = 0.5, then top_logprobs would contain log probabilities for top 50% of generated tokens."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    logprobs=2)"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431290842
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "Completion(id='cmpl-8MjelXkZYFARm1oLOqhG8p0UPp6lv', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=Logprobs(text_offset=[18, 21, 26, 30, 40, 49, 56, 57, 62, 64, 77, 85, 89, 99, 109, 118], token_logprobs=[-3.2129984, -0.92528147, -0.19372824, -2.7829094, -0.13028544, -4.4900484, -2.2128243, -0.41160226, -1.2931756, -5.8501472, -1.5920293, -1.9686843, -3.6532416, -2.7601342, -0.57346404, -1.6445538], tokens=['150', ' year', ' old', ' financial', ' services', ' Mutual', ',', ' with', ' a', ' longstanding', ' history', ' and', ' excellent', ' financial', ' strength', '.'], top_logprobs=[{'100': -2.564561, '200': -2.6895611}, {'-year': -0.62840635, ' year': -0.92528147}, {' old': -0.19372824, '-old': -1.7718532}, {' mutual': -0.59540945, ' company': -2.1110344}, {' services': -0.13028544, ' institution': -3.5834103}, {' company': -0.31817323, ' mutual': -1.5681733}, {'.': -1.1034495, ' with': -2.0878243}, {' with': -0.41160226, ' which': -2.3334773}, {' over': -1.1369256, ' a': -1.2931756}, {' history': -2.4282727, ' vision': -2.475148}, {' reputation': -0.9201543, ' commitment': -1.5295293}, {' of': -0.34368432, ' and': -1.9686843}, {' reputation': -1.6532418, ' a': -1.6688668}, {' reputation': -0.52575916, ' customer': -2.400759}, {' strength': -0.57346404, ' ratings': -1.4797139}, {' ratings': -0.6758039, '.': -1.6445538}]), text='150 year old financial services Mutual, with a longstanding history and excellent financial strength.')], created=1700429411, model='gpt-35-turbo-instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=5, total_tokens=21))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700429524981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word values :\",response.choices[0].logprobs.tokens)\n",
        "print(\"Token log probabilities :\",response.choices[0].logprobs.token_logprobs)\n",
        "#print(\"Top log probabilities :\",response.choices[0].logprobs.top_logprobs)\n",
        "\n",
        "print(\"Response:\", response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Word values : ['164', ' year', ' old', ' mutual', ' life', ' assurance', ',', ' pensions', ' and', ' investment', ' company', '.', ' It', ' offers', ' life', ' and']\nToken log probabilities : [-8.423936, -1.575275, -0.57227653, -1.0111781, -0.62081426, -2.9730442, -0.3743286, -0.08711771, -0.115021884, -0.08659351, -0.07886103, -0.81804967, -1.7060416, -3.6840808, -3.9320595, -0.5578528]\nResponse: 164 year old mutual life assurance, pensions and investment company. It offers life and\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431295907
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presence_penalty\n",
        "\n",
        "Defaults to 0, Optional -> 0 means there is really no penalty or reward for the same token appearing multiple times. \n",
        "\n",
        "Number between -2.0 and 2.0. Positive values penalize new tokens based on ** whether they appear in the text so far **, increasing the model's likelihood to talk about new topics.\n",
        "\n",
        "Smaller values (minimum -2) decrease the penalty and increase the chances of a token appearing, while higher values (maximum 2) increase the penalty and decrease the chances of a token appearing."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    presence_penalty=2,\n",
        "    max_tokens=50)"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430628115
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "4-star comfortable hotel providing a spacious accommodation only minutes away from Shakespeare's Birthplace, Warwick Castle and Royal Shakespeare Theatre. Since 1830, it has been hosting guests in the heart of Stratford-upon-Avon.\n\nBridgehouse Guest\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430629039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    presence_penalty=-2,\n",
        "    max_tokens=50)\n",
        "\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "179-acre property on spectacular Horse Island, nestled in Roaring Water Bay on the rugged West Coast of Ireland. Surrounded by the blue waters of the Atlantic, Horse Island, with its many coves, beaches, cliffs, woodlands, fields\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430658848
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.model_dump_json(indent=2))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\n  \"id\": \"cmpl-8MkHRsPktfy2IKPayAM00rbJ48rEN\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \"_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar_mar\"\n    }\n  ],\n  \"created\": 1700431809,\n  \"model\": \"gpt-35-turbo-instruct\",\n  \"object\": \"text_completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 50,\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 55\n  }\n}\n"
        }
      ],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432108916
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Frequency_penalty\n",
        "\n",
        "Defaults to 0\n",
        "\n",
        "Number between -2.0 and 2.0. Positive values ** penalize new tokens based on their existing frequency in the text so far **, decreasing the model's likelihood to repeat the same line verbatim."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Best_of\n",
        "\n",
        "Defaults to 1,optional\n",
        "\n",
        "This parameter tells the language model to generate multiple completions and return the best one, which is the one with the highest log probability per token.\n",
        "\n",
        "\n",
        "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase, \n",
        "    best_of=3,\n",
        "    max_tokens=50)\n",
        "\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Province in the North of England. It is bordered by Northumberland to the north, County Durham to the south, Cumbria to the west, and the North Sea to the east.\n\nThe origins of the name \"Royal London\" can be traced\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430873851
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logit_bias\n",
        "\n",
        "Defaults to null\n",
        "\n",
        "The logit_bias request parameter is used to modify the likelihood of specified tokens appearing in the completion. We can use this parameter to provide hints to the language model about which tokens we want or don’t want to appear in the completion. It basically allows us to make the model more biased towards certain keywords or topics."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-100 bias, which should completely prevent them from appearing.\n",
        "# 100 bias will show only that word\n",
        "start_phrase = 'Royal London is a '\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase,\n",
        "    max_tokens=50,\n",
        "    logit_bias={\"30\":-100, \"5936\": 100})\n",
        "#5936 for April\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April April\n"
        }
      ],
      "execution_count": 63,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432166815
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Echo and Stop"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting the echo parameter to true, you’re asking the language model to return the prompt embedded within the completion. This is useful for debugging."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London Group is a '\n",
        "\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase,\n",
        "    max_tokens=50,\n",
        "    echo=True)\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Royal London Group is a 200 year-old life assurance company and one of the UK’s largest providers of pensions and insurance. Headquartered in the City of London it has grown rapidly since merger with United Assurance in 1995 and through a number of strategic acquisitions in recent years.\n"
        }
      ],
      "execution_count": 67,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432612812
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ** stop ** parameter allows you to specify up to 4 sequences of text on which the language model will halt and return the result. This is useful for specifying early termination triggers for the language model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London Group is a '\n",
        "\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase,\n",
        "    max_tokens=50,\n",
        "    temperature=0,\n",
        "    stop=\"company\")\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "150-year-old mutual life and pensions \n"
        }
      ],
      "execution_count": 71,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432779810
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_phrase = 'Royal London Group is a '\n",
        "\n",
        "response = client.completions.create(\n",
        "    model=deployment_name, \n",
        "    prompt=start_phrase,\n",
        "    max_tokens=50,\n",
        "    temperature=0,\n",
        "    stop=[\"company\",\"pensions\"])\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "150-year-old mutual life and \n"
        }
      ],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432799803
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}