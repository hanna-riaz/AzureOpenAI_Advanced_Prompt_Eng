{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with prompty"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
        "\n",
        "- Write LLM application using prompty and visualize the trace of your application.\n",
        "- batch run prompty against multi lines of data.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Install dependent packages"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install promptflow-core"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Execute a Prompty\n",
        "\n",
        "Prompty is a file with .prompty extension for developing prompt template. \n",
        "The prompty asset is a markdown file with a modified front matter. \n",
        "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"prompty/basic.prompty\") as fin:\n",
        "    print(fin.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "---\nname: Basic Prompt\ndescription: A basic prompt that uses the chat API to answer questions\nmodel:\n    api: chat\n    configuration:\n        type: azure_openai\n        azure_deployment: gpt-4-0125-Preview\n    parameters:\n        max_tokens: 128\n        temperature: 0.2\ninputs:\n  question:\n    type: string\nsample:\n  \"question\": \"Who is the most famous person in the world?\"\n---\nsystem:\nYou are an AI assistant who helps people find information.\nAs the assistant, you answer questions briefly, succinctly. \n\nuser:\n{{question}}\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1720054652868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"credentials.env\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1720054662893
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from promptflow.core import Prompty\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\")\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"What is the capital of France?\")\n",
        "result"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "'The capital of France is Paris.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1720054682828
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can override configuration with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
        "\n",
        "# override configuration with AzureOpenAIModelConfiguration\n",
        "configuration = AzureOpenAIModelConfiguration(\n",
        "    # azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
        "    # api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
        "    azure_deployment=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "# override configuration with OpenAIModelConfiguration\n",
        "# configuration = OpenAIModelConfiguration(\n",
        "#     base_url=\"${env:OPENAI_BASE_URL}\",\n",
        "#     api_key=\"${env:OPENAI_API_KEY}\",\n",
        "#     model=\"gpt-3.5-turbo\"\n",
        "# )\n",
        "\n",
        "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\", model=override_model)\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"What is the capital of France?\")\n",
        "result"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "'The capital of France is Paris.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1720054702823
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize trace by using start_trace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from promptflow.tracing import start_trace\n",
        "\n",
        "# start a trace session, and print a url for user to check trace\n",
        "start_trace()"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1720054713877
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-run below cell will collect a trace in trace UI."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# rerun the function, which will be recorded in the trace\n",
        "question = \"What is the capital of Japan?\"\n",
        "ground_truth = \"Tokyo\"\n",
        "result = f(question=question)\n",
        "result"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "'Tokyo.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1720054716833
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval the result \n",
        "\n",
        "Note: the eval flow returns a `json_object`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load prompty as a flow\n",
        "eval_flow = Prompty.load(\"prompty/eval.prompty\")\n",
        "# execute the flow as function\n",
        "result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
        "result"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "{'score': '5', 'explanation': 'Tokyo is the capital of Japan.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1720054722834
        }
      }
    }
  ],
  "metadata": {
    "resources": "examples/requirements.txt, examples/prompty/basic, examples/prompty/eval-basic",
    "build_doc": {
      "author": [
        "lalala123123@github.com",
        "wangchao1230@github.com"
      ],
      "category": "local",
      "section": "Prompty",
      "weight": 10
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "description": "A quickstart tutorial to run a prompty and evaluate it.",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}